{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images...\n",
      "Normalizing images...\n",
      "Separating train and test sets\n",
      "Train data dimension: (8071, 128, 128, 3)\n",
      "Test data dimension: (896, 128, 128, 3)\n",
      "Train labels dimension: (8071,)\n",
      "Test labels dimension: (896,)\n",
      "INFO:tensorflow:Restoring parameters from ./tmp/DMF/cnnData2\n",
      "Model restored from ./tmp/DMF/cnnData2\n",
      "loaded model\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 12288 into shape (3,128,128,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-dd67e6e78f2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m   \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\AdvML\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(main, argv)\u001b[0m\n\u001b[0;32m     46\u001b[0m   \u001b[1;31m# Call the main function, passing through any arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m   \u001b[1;31m# to the final program.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m   \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mflags_passthrough\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-dd67e6e78f2d>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(unused_argv)\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tmp/DMF/eval_data_dmf.npy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m             \u001b[0mactivations_feature1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbest_feature1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_index_feature1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDeconvNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbestActivation1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0meval_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_feature1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m             \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tmp/MNIST/ActivationsDMF_Layer1_Features.npy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivations_feature1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\MLProject\\lib\\ConvDeconvDataSet2.py\u001b[0m in \u001b[0;36mbestActivation1\u001b[1;34m(self, inputImage, inputLabel, n_best, k)\u001b[0m\n\u001b[0;32m    422\u001b[0m                 \u001b[0mbest_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbest\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m                 \u001b[0misolated\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misolated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_best\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m                 \u001b[0mall_isolations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misolated\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\AdvML\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    230\u001b[0m            [5, 6]])\n\u001b[0;32m    231\u001b[0m     \"\"\"\n\u001b[1;32m--> 232\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reshape'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\AdvML\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 12288 into shape (3,128,128,3)"
     ]
    }
   ],
   "source": [
    "##\n",
    "## main \n",
    "##\n",
    "\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "from PIL import Image\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "# Import script with auxiliar functions\n",
    "import ConvDeconvDataSet2 as nets\n",
    "\n",
    "\n",
    "def main(unused_argv):\n",
    "\n",
    "  ############################################\n",
    "  ## Load Dog, Muffin, Fried Chicken data\n",
    "  ############################################\n",
    "\n",
    "  # Load training and eval data\n",
    "#   mnist        = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "#   train_data   = mnist.train.images[0:1000] # Returns np.array - reading only 1000 images\n",
    "#   train_labels = np.asarray(mnist.train.labels[0:1000], dtype=np.int32) # - reading only 1000 images\n",
    "#   eval_data    = mnist.test.images[0:200] # Returns np.array - reading only 200 images\n",
    "#   eval_labels  = np.asarray(mnist.test.labels[0:200], dtype=np.int32) # - reading only 200 images\n",
    "\n",
    "  #########################################\n",
    "  ##RESIZE IMAGES\n",
    "  #########################################\n",
    "\n",
    "\n",
    "   # path where files are \"../data/dataset2/train\"\n",
    "#   train_path = \"data/dataset2/train/\"\n",
    "#   resized_train_path = \"data/dataset2/train_resized/\"\n",
    "#   files = [train_path + f for f in os.listdir(train_path) if f.endswith('.jpg')]\n",
    "#   npixels = 128\n",
    "#   for fl in files:\n",
    "#     print(\"Reading: \")\n",
    "#     print( fl )\n",
    "#\n",
    "#     # define file name\n",
    "#     splitName = fl.split(\"/\")\n",
    "#     fileName  = splitName[len(splitName)-1]\n",
    "#     # open\n",
    "#     img = Image.open(fl)\n",
    "#     img = img.resize((npixels,npixels), Image.ANTIALIAS)\n",
    "#     img.save( resized_train_path + fileName )\n",
    "#\n",
    "#\n",
    "#   ############################################\n",
    "#   ### Read resized images and store in np.array\n",
    "#   ############################################\n",
    "#   #path with resized images\n",
    "#   resized_train_path = \"../data/dataset2/train_resized/\"\n",
    "#   files = [resized_train_path + f for f in os.listdir(resized_train_path) if f.endswith('.jpg')]\n",
    "#   images = []\n",
    "#   images_flip = []\n",
    "#   images_rot  = []\n",
    "#   notRGB = []\n",
    "#   i = 0\n",
    "#   for fl in files:\n",
    "#\n",
    "#     # for seed\n",
    "#     i = i + 1\n",
    "#\n",
    "#     # open image\n",
    "#     img = Image.open( fl )\n",
    "#\n",
    "#     if img.mode == \"RGB\":\n",
    "#\n",
    "#       # flip image\n",
    "#       flipped = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "#       img_flp = np.array( flipped )\n",
    "#       #img_flp = np.multiply( img_flp, 1.0 / 255.0 )\n",
    "#       images_flip.append( img_flp )\n",
    "#\n",
    "#\n",
    "#       # rotate image\n",
    "#       random.seed(i)\n",
    "#       angle = np.random.uniform(-15,15)\n",
    "#       rotated = img.rotate( angle )\n",
    "#       img_rot = np.array( rotated )\n",
    "#       #img_rot = np.multiply( img_rot, 1.0 / 255.0 )\n",
    "#       images_rot.append( img_rot )\n",
    "#\n",
    "#\n",
    "#       # regular image\n",
    "#       img_array = np.array( img )\n",
    "#       #img_array = np.multiply( img_array, 1.0 / 255.0 )\n",
    "#       images.append( img_array )\n",
    "#\n",
    "#       # close image\n",
    "#       img.close()\n",
    "#\n",
    "#     else:\n",
    "#       notRGB.append( fl )\n",
    "#\n",
    "#\n",
    "#   train_normal  = np.array( images )\n",
    "#   train_flipped = np.array( images_flip )\n",
    "#   train_rotated = np.array( images_rot )\n",
    "#   notRGB        = np.array( notRGB )\n",
    "#\n",
    "#   print(\"train_normal shape\")\n",
    "#   print(train_normal.shape)\n",
    "#   print(\"train_flipped shape\")\n",
    "#   print(train_flipped.shape)\n",
    "#   print(\"train_rotated shape\")\n",
    "#   print(train_rotated.shape)\n",
    "#   print(\"notRGB files\")\n",
    "#   print(notRGB)\n",
    "#\n",
    "##   #save training data (3 np arrays)\n",
    "#   np.save(\"../data/dataset2/train_data_normal.npy\", train_normal )\n",
    "#   np.save(\"../data/dataset2/train_data_flipped.npy\", train_flipped )\n",
    "#   np.save(\"../data/dataset2/train_data_rotated.npy\", train_rotated )\n",
    "#   np.savetxt(\"../data/dataset2/train_data_notRGB.txt\", notRGB, fmt='%s')\n",
    "#\n",
    "\n",
    "  ###\n",
    "  ### Load data\n",
    "  ###\n",
    "\n",
    "  \n",
    "  # Load images\n",
    "    print(\"Loading images...\")\n",
    "    train_normal  = np.load(\"../data/dataset2/train_data_normal.npy\" )\n",
    "    train_flipped = np.load(\"../data/dataset2/train_data_flipped.npy\" )\n",
    "    train_rotated = np.load(\"../data/dataset2/train_data_rotated.npy\" )\n",
    "\n",
    "  # stacking in a single np.array\n",
    "    train_all = np.concatenate( (train_normal, train_flipped, train_rotated) )\n",
    "\n",
    "  # normaliz\n",
    "    print(\"Normalizing images...\")\n",
    "    train_all = train_all[ None:, :, :, :,] * (1.0 / 255.0)\n",
    "    \n",
    "  # normalize all values \n",
    "    #train_all = np.multiply( train_all[0:0,:,:,:], 1/255.0 )\n",
    "    #print(train_all.shape)\n",
    "  # Load file names that are not RGB (to be excluded from train data)\n",
    "    indexNot = ['379', '657', '1033', '1124', '1154', '1164', '1201', '1289', '2154', '2287', '2428']\n",
    "\n",
    "  # Load labels \n",
    "    with open(\"../data/dataset2/label_train.csv\", mode='r') as infile:\n",
    "        reader = csv.reader(infile, delimiter = \";\")\n",
    "        lbls   = {rows[0]:rows[1] for rows in reader}\n",
    "\n",
    "  # Remove unwanted images\n",
    "    for k in indexNot:\n",
    "        lbls.pop(k, None)\n",
    "\n",
    "\n",
    "    print(\"Separating train and test sets\")\n",
    "    random.seed(1)\n",
    "    subset = random.sample(range(8967), 896 ) # 3 * 2989 = 8967 total input size\n",
    "\n",
    "    train_data = np.delete(train_all, subset, 0)\n",
    "    eval_data = train_all[subset,]\n",
    "  \n",
    "    print(\"Train data dimension:\", train_data.shape)\n",
    "    print(\"Test data dimension:\", eval_data.shape)\n",
    "    #Put labels in array format\n",
    "    labels_load = np.array(list(lbls.values()))\n",
    "    labels_all  = np.concatenate( (labels_load, labels_load, labels_load) )\n",
    "\n",
    "  \n",
    "    train_labels = np.delete(labels_all, subset, 0)\n",
    "    eval_labels = labels_all[subset,]\n",
    "  \n",
    "    print(\"Train labels dimension:\", train_labels.shape)\n",
    "    print(\"Test labels dimension:\", eval_labels.shape)\n",
    "\n",
    "#   # print( images.shape )\n",
    "#   #plt.imshow(np.array(a1[1,:,:,0]), cmap='gray')\n",
    "#   #plt.show()\n",
    "\n",
    "  ############################################\n",
    "  ## Run session\n",
    "  ############################################\n",
    "    with tf.Graph().as_default():\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            train_labels = train_labels.astype( np.int )\n",
    "            eval_labels = eval_labels.astype( np.int )\n",
    "            batch_size   = 48\n",
    "            max_iter     = 1000\n",
    "\n",
    "\n",
    "#         # instatiate Network\n",
    "#         net = nets.CnnData2( sess, filterSizeConv1 = 3, nFiltersConv1 = 32, \n",
    "#        filterSizeConv2 = 3, nFiltersConv2 = 32,\n",
    "#        filterSizeConv3 = 3, nFiltersConv3 = 64,\n",
    "#        fcUnits = 256 ) \n",
    "#\n",
    "#         # usual tf initialization\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "#\n",
    "#         # make labels one-hot encoded\n",
    "#         onehot_labels_train = tf.one_hot( indices = tf.cast(train_labels, tf.int32), depth =  3 ).eval()\n",
    "#\n",
    "#         # print error rate before training\n",
    "#         #print('error rate BEFORE training is {}'.format((np.sum(net.compute(train_data)!=train_labels) / train_labels.size)))\n",
    "#\n",
    "#         # train network\n",
    "#         #net.train( train_data, onehot_labels_train )\n",
    "#\n",
    "#         # now train...\n",
    "#\n",
    "#         print(\"Started training...\")\n",
    "#         start = time.time()\n",
    "#         \n",
    "#         for i in range( max_iter ):\n",
    "#\n",
    "#           batch = random.sample( range(train_data.shape[0]), batch_size )\n",
    "#           x_batch = train_data[batch]\n",
    "#           y_batch = onehot_labels_train[batch]\n",
    "#           net.train( x_batch,  y_batch )\n",
    "#           if i % 100 == 0:\n",
    "#             print('Aproximate error rate at iteration {} is {} %'.format(i, ( np.sum(net.compute(train_data[0:100,])!=train_labels[0:100]) / train_labels[0:100].size *100)))\n",
    "#\n",
    "#         elapsed = (time.time() - start)\n",
    "#         \n",
    "#         print(\"Total training time: {} seconds\".format(round(elapsed,2)))\n",
    "#            \n",
    "#         print('Aproximate Final training error is {} %'.format((np.sum(net.compute(train_data[0:500])!=train_labels[0:500]) / train_labels[0:500].size *100)))\n",
    "#        \n",
    "#         print('Test error is {} %'.format(( np.sum(net.compute(eval_data)!=eval_labels) / eval_labels.size *100)))\n",
    "#\n",
    "#\n",
    "#        # # save the trained network \n",
    "#         net.netSaver(\"./tmp/DMF/cnnData2\")\n",
    "          \n",
    "        \n",
    "        ##\n",
    "        ## Deconvolution Part - until here it runs OK\n",
    "        ##\n",
    "        \n",
    "        # load trained model\n",
    "            net = nets.CnnData2( sess, filterSizeConv1 = 3, nFiltersConv1 = 32, \n",
    "                             filterSizeConv2 = 3, nFiltersConv2 = 32, filterSizeConv3 = 3, \n",
    "                             nFiltersConv3 = 64, fcUnits = 256  ) \n",
    "         \n",
    "            net.netLoader( \"./tmp/DMF/cnnData2\" )\n",
    "\n",
    "            print( \"loaded model\" )\n",
    "\n",
    "\n",
    "         # instantiate deconv net\n",
    "            DeconvNet = net.createDeconvNet( eval_data[:100,], eval_labels[:100] )\n",
    "\n",
    "#         print( \"\\nDimension of input data\")\n",
    "#         print( train_data.shape)\n",
    "#\n",
    "#         print( \"\\nNumber of Images\")\n",
    "#         print( train_data.shape[0])\n",
    "#\n",
    "#         dec1, dec2, dec3  = DeconvNet.getDeconv()\n",
    "#\n",
    "#         print( \"\\nDimension of Deconvoluted images - Layer 1\")\n",
    "#         print( dec1.shape )\n",
    "#         print( dec1 )\n",
    "#\n",
    "#         print( \"\\nDimension of Deconvoluted images - Layer 2\")\n",
    "#         print( dec2.shape )\n",
    "#         print( dec2 )\n",
    "#\n",
    "#         print( \"\\nDimension of Deconvoluted images - Layer 3\")\n",
    "#         print( dec3.shape )\n",
    "#         print( dec3 )\n",
    "                 \n",
    "         #conv1, conv2, conv3 = net.getConvs()\n",
    "        #\n",
    "         ##Images within Layers (Aman)\n",
    "        #\n",
    "         #images_within_layer1 = net.activate( conv1, eval_data[0,], sess)\n",
    "         #np.save(\"tmp/DMF/FilterProjectionDMF_Layer1.npy\", images_within_layer1)\n",
    "        \n",
    "         #images_within_layer2 = net.activate( conv2, eval_data[0,], sess)\n",
    "         #np.save(\"tmp/DMF/FilterProjectionDMF_Layer2.npy\", images_within_layer2)\n",
    "         \n",
    "         #images_within_layer3 = net.activate( conv3, eval_data[0,], sess)\n",
    "         #np.save(\"tmp/DMF/FilterProjectionDMF_Layer3.npy\", images_within_layer3)\n",
    "         \n",
    "         \n",
    "#        #a1 = dec1.eval()\n",
    "#        #plt.imshow(np.array(a1[0,:,:,0]) )\n",
    "#        #plt.show()\n",
    "#        \n",
    "#        #Activations part----------------------------\n",
    "#\n",
    "            np.save(\"tmp/DMF/eval_data_dmf.npy\", eval_data[:100,])\n",
    "             \n",
    "            activations_feature1,best_feature1, filter_index_feature1=DeconvNet.bestActivation1(eval_data[:100],eval_labels[:100])\n",
    "            print(best_feature1)\n",
    "            np.save(\"tmp/MNIST/ActivationsDMF_Layer1_Features.npy\", activations_feature1)\n",
    "            np.save(\"tmp/MNIST/BestImagesDMF_Layer1_Features.npy\", best_feature1)\n",
    "            np.save(\"tmp/MNIST/RandomFiltersIndexDMF_Layer1_Features.npy\", filter_index_feature1)\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            activations_feature2,best_feature2,filter_index_feature2=DeconvNet.bestActivation2(eval_data[:100],eval_labels[:100])\n",
    "            print(best_feature2)\n",
    "            np.save(\"tmp/MNIST/ActivationsDMF_Layer2_Features.npy\", activations_feature2)\n",
    "            np.save(\"tmp/MNIST/BestImagesDMF_Layer2_Features.npy\", best_feature2)\n",
    "            np.save(\"tmp/MNIST/RandomFiltersIndexDMF_Layer2_Features.npy\", filter_index_feature2)\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            activations_feature3,best_feature3, filter_index_feature3 =Deconvnet.bestActivation3(eval_data[:100],eval_labels[:100])\n",
    "            print(best_feature3)\n",
    "            np.save(\"tmp/MNIST/ActivationsDMF_Layer3_Features.npy\", activations_feature3)\n",
    "            np.save(\"tmp/MNIST/BestImagesDMF_Layer3_Features.npy\", best_feature3)\n",
    "            np.save(\"tmp/MNIST/RandomFiltersIndexDMF_Layer3_Features.npy\", filter_index_feature3)\n",
    "            print(\"\\n\")\n",
    "            a1, b1, c1 = DeconvNet.displayFeatures1(eval_data[:100,], eval_labels[:100], n_best = 3, k = 3)\n",
    "            np.save(\"tmp/DMF/ActivationsDMF_Layer1.npy\", a1)\n",
    "            np.save(\"tmp/DMF/BestImagesDMF_Layer1.npy\", b1)\n",
    "            np.save(\"tmp/DMF/RandomFiltersIndex_Layer1.npy\", c1)\n",
    "            print(bq)\n",
    "        \n",
    "            print(\"\\n\")\n",
    "            a2, b2, c2 = DeconvNet.displayFeatures2(eval_data[:100,], eval_labels[:100], n_best = 3, k = 3)\n",
    "            np.save(\"tmp/DMF/ActivationsDMF_Layer2.npy\", a2)\n",
    "            np.save(\"tmp/DMF/BestImagesDMF_Layer2.npy\", b2)\n",
    "            np.save(\"tmp/DMF/RandomFiltersIndex_Layer2.npy\", c2)\n",
    "            print(a2.shape)  \n",
    "        \n",
    "            print(\"\\n\")\n",
    "            a3, b3, c3 = DeconvNet.displayFeatures2(eval_data[:100,], eval_labels[:100], n_best = 3, k = 3)\n",
    "            np.save(\"tmp/DMF/ActivationsDMF_Layer3.npy\", a3)\n",
    "            np.save(\"tmp/DMF/BestImagesDMF_Layer3.npy\", b3)\n",
    "            np.save(\"tmp/DMF/RandomFiltersIndex_Layer3.npy\", c3)\n",
    "            print(a3.shape)     \n",
    "#        \n",
    "#         #Weights part--------------------------------\n",
    "#        \n",
    "#         w1_t, w2_t, w3_t = net.getWeights()\n",
    "#        \n",
    "#         w1 = w1_t.eval()\n",
    "#         np.save(\"tmp/DMF/WeightDMF_1.npy\", w1)\n",
    "#        \n",
    "#         w2 = w2_t.eval()\n",
    "#         np.save(\"tmp/DMF/WeightDMF_2.npy\", w2)\n",
    "#        \n",
    "#         w3 = w3_t.eval()\n",
    "#         np.save(\"tmp/DMF/WeightDMF_3.npy\", w3)\n",
    "\n",
    "         # plt.imshow(np.array1(train_data[1,:,:,0]), cmap='gray')\n",
    "         # plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
